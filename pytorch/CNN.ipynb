{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model for Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we approach text classification problem with a CNN model. The dataset used for this project can be found [here](https://github.com/andreasceid/sentiment_classification/blob/main/dataset/MoviesDataset.csv). The accuracy of the defined model on this dataset is approximately 77%. However, the model was also tested with the IMDB dataset which can be found [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). The accuracy improved over the IMDB dataset and reached approximately 91%. The variable to be classified is the sentiment of the given critics. The critics are separated in *good* and *bad*, which means that the classification is binary. Let's explore the proposed model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import spacy\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "The model is composed of:\n",
    "* An Embedding Layer.\n",
    "* (Four) 2D Convolutional Layers.\n",
    "* A Dropout Layer.\n",
    "* A Fully Connected Layer.\n",
    "\n",
    "The Embedding Layer is used as a lookup table for each token. Therefore, because the critics consist of many words, a matrix is shaped with those word vectors. This matrix can is our *image*. On that image, we apply the filters of the four *Convolutional* layers. The filtered results are then used as an input for a *Fully Connected* layer that implements the (binary) sentiment classification. To regularize the model, we also apply a *Dropout* layer. \n",
    "\n",
    "![Embedding Demo](https://miro.medium.com/max/501/1*A094Vuq3OiLFVD2ogxUS7Q.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        # extends the functionality of this method\n",
    "        super(CNN, self).__init__()\n",
    "        # defines an embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # freezes the embedding layer\n",
    "        self.embedding.requires_grad = False\n",
    "        # applies convolution over the input signal\n",
    "        self.convs_1d = nn.ModuleList([nn.Conv2d(1, n_filters, (k, embedding_dim), padding=(k - 2, 0)) for k in filter_sizes])\n",
    "        # applies linear transformation to the convolved data\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        # regularizes and prevents the co-adaptation of neurons\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_and_pool(x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedded vectors of: (batch_size, seq_length, embedding_dim)\n",
    "        embeds = self.embedding(x)\n",
    "        # creates a fourth dimension for the convolutional module list\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "        # gets output of each convolutional layer\n",
    "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
    "        # concatenates results\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        # add dropout\n",
    "        x = self.dropout(x)\n",
    "        # fully connected layer that yields a float tensor of size equal to the batch size\n",
    "        logit = self.fc(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "In this project, the dataset provided for *Sentiment Classification* is already preprocessed. There are no capital letters and any punctuation marks are already removed. Therefore, the methods defined below do not increase the model's accuracy. However, if one uses the IMDB dataset for sentiment classification, then these methods might actually boost the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?[)(DP]', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preprocessor(df, column, filepath):\n",
    "    # apply the preprocessor to the dataframe\n",
    "    df[column] = df[column].apply(nlp_preprocessor)\n",
    "    # save data\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Dataset\n",
    "\n",
    "The train-validate-test split is a technique for evaluating the performance of a machine learning model. It can be used for any supervised learning algorithm. The procedure involves taking a dataset and dividing it into three subsets:\n",
    "* Training Dataset: The sample of data used to fit the model.\n",
    "* Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "* Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "With this technique, we prevent look-ahead bias, overfitting and underfitting:\n",
    "* Look-ahead bias: Building a model based on data that is not supposed to be known.\n",
    "* Overfitting: This is the process of designing a model that adapts so closely to historical data that it becomes ineffective in the future.\n",
    "* Underfitting: This is the process of designing a model that adapts so loosely to historical data that it becomes ineffective in the future.\n",
    "\n",
    "Here, we illustrate the process of the train-validate-test split technique \n",
    "\n",
    "![train-validate-test Demo](https://miro.medium.com/max/1200/1*HEe_oHZHToY8oD1RoShHGg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, seed, train_percent=.7, validate_percent=.1):\n",
    "    # shuffle the given dataframe indexes\n",
    "    shuffled = numpy.random.RandomState(seed).permutation(df.index)\n",
    "    # get the number of rows inside the dataframe\n",
    "    data_length = len(df.index)\n",
    "    # compute the number of rows for the training dataset\n",
    "    train_end = int(train_percent * data_length)\n",
    "    # make the training dataset size divide perfectly the batch size\n",
    "    train_end = int(train_end/BATCH_SIZE) * BATCH_SIZE + BATCH_SIZE\n",
    "    # compute the number of rows for the validation dataset\n",
    "    validate_end = int(validate_percent * data_length) + train_end\n",
    "    # make the validation dataset size divide perfectly the batch size\n",
    "    validate_end = int(validate_end / BATCH_SIZE) * BATCH_SIZE + BATCH_SIZE\n",
    "    # make the test dataset size divide perfectly the batch size\n",
    "    test_end = int(data_length / BATCH_SIZE) * BATCH_SIZE\n",
    "    # set the training dataset\n",
    "    train_df = df.iloc[shuffled[:train_end]]\n",
    "    # set the validation dataset\n",
    "    valid_df = df.iloc[shuffled[train_end:validate_end]]\n",
    "    # set the test dataset\n",
    "    test_df = df.iloc[shuffled[validate_end:test_end]]\n",
    "    # save the training dataset\n",
    "    train_df.to_csv('train_df.csv', index=False)\n",
    "    # save the validation dataset\n",
    "    valid_df.to_csv('valid_df.csv', index=False)\n",
    "    # save the test dataset\n",
    "    test_df.to_csv('test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary inspection\n",
    "\n",
    "In the given dataset, there are tokens that are found only a few times. Those tokens have to be excluded from the classification task. The reason is that, the classifier will not be able to find a pattern due to insufficient data on the given token. On this matter, there are vocabulary preprocessing techniques:\n",
    "* Subsampling frequent words\n",
    "* Deleting rare words\n",
    "\n",
    "We combine the $2$ methods and come up with a filtering formula:\n",
    "\n",
    "$$\n",
    "p \\; = \\; 1 \\; - \\; \\sqrt{\\frac{t}{f}}\n",
    "$$\n",
    "where:\n",
    "* $p$ is a factor that determines whether a rare word is to be included in the dictonary or not. If $p$ is greater than $0.5$, then there are enough data on that word and therefore it shall be included in the vocabulary.\n",
    "* $t$ is a threshold that based on [research](https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00134) is initialized at $10^{-5}$.\n",
    "* $f$ is the frequency of the token. The frequency is equal to $\\frac{\\text{number of token appearences}}{\\text{number of unique tokens}}$.\n",
    "\n",
    "With this method, the *context window* size increases. The context window is the area around a vertex that is used to encode an embedding. This usually provided slightly better results regarding the preprocessed word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_vocab(df):\n",
    "    # initialize vocabulary size register\n",
    "    unique_count = 0\n",
    "    # get the text column loaded in a pandas Series\n",
    "    texts = df.Summary.str.lower()\n",
    "    # get a dictionary with the count of each token in that pandas Series\n",
    "    word_counts = Counter(word_tokenize('\\n'.join(texts)))\n",
    "    # get the total token sum\n",
    "    total_token_count = sum(word_counts.values())\n",
    "    # get the unique token sum\n",
    "    final_count = len(word_counts)\n",
    "    # initialize threshold constant\n",
    "    threshold = 1e-5\n",
    "    # use the subsampling formula to estimate the vocabulary size\n",
    "    for token_freq in word_counts.values():\n",
    "        if 1 - math.sqrt(threshold / (token_freq / total_token_count)) > 0.5:\n",
    "            unique_count += 1\n",
    "    return unique_count, final_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vocab_size():\n",
    "    if subsampling:\n",
    "        return vocab_subsampled\n",
    "    else:\n",
    "        return token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Throughout the notebook, we use some helper methods:\n",
    "* We define a method that counts all the trainable parameters found in the model. This helps us estimate the performance of the model. The more trainable parameters a model has, the worse performance it usually has. We prefer lightweight models for aout classification tasks.\n",
    "* We define a method that estimates the model's accuracy in binary classification task.\n",
    "* We define a method that finds the maximum sentence length found in the given dataset. This is useful when the *text preprocessor* adds *padding* to sentences. However, for this dataset (and for the IMDB) dataset, there is no such need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters():\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    # use the sigmoid to round the predictions\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    # count the correct predictions by comparing them to the ground truth tensor\n",
    "    correct = (rounded_preds == y).float()\n",
    "    # compute the accuracy of the model\n",
    "    acc = correct.sum() / len(correct)\n",
    "    # return the accuracy\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(df):\n",
    "    # initializes maximum sentence length register\n",
    "    max_len = 0\n",
    "    # iterates the Summary column of the given dataframe\n",
    "    for text in df.Summary:\n",
    "        # checks length of the \"running\" sentence\n",
    "        if len(text.split()) > max_len:\n",
    "            # update maximum sentence length register\n",
    "            max_len = len(text.split())\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "To fit the model, there are $2$ methods defined. The first method trains the model and the second evaluates it. There are also additional methods that:\n",
    "* Estimate each epoch's duration\n",
    "* Plot model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterator):\n",
    "    # initializes epoch loss accumulator\n",
    "    epoch_loss = 0\n",
    "    # initializes epoch accuracy accumulator\n",
    "    epoch_acc = 0\n",
    "    # sets the module in training mode\n",
    "    model.train()\n",
    "    for batch in tqdm(iterator, desc=\"Train\"):\n",
    "        # set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # make predictions\n",
    "        predictions = model(batch.Summary).squeeze(1)\n",
    "        # compute loss\n",
    "        loss = criterion(predictions, batch.Sentiment.squeeze(0))\n",
    "        # compute accuracy\n",
    "        acc = binary_accuracy(predictions, batch.Sentiment.squeeze(0))\n",
    "        # store the gradients\n",
    "        loss.backward()\n",
    "        # parameter update based on the current gradients\n",
    "        optimizer.step()\n",
    "        # update epoch loss accumulator\n",
    "        epoch_loss += loss.item()\n",
    "        # update epoch accuracy accumulator\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(iterator):\n",
    "    # initializes epoch loss accumulator\n",
    "    epoch_loss = 0\n",
    "    # initializes epoch accuracy accumulator\n",
    "    epoch_acc = 0\n",
    "    # sets the module in evaluation mode\n",
    "    model.eval()\n",
    "    # disables gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator, desc=\"Validate\"):\n",
    "            # make predictions\n",
    "            predictions = model(batch.Summary).squeeze(1)\n",
    "            # compute loss\n",
    "            loss = criterion(predictions, batch.Sentiment.squeeze(0))\n",
    "            # compute accuracy\n",
    "            acc = binary_accuracy(predictions, batch.Sentiment.squeeze(0))\n",
    "            # update epoch loss accumulator\n",
    "            epoch_loss += loss.item()\n",
    "            # update epoch accuracy accumulator\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time():\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy():\n",
    "    plt.plot(train_losses, label=\"Training loss\")\n",
    "    plt.plot(val_losses, label=\"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Losses\")\n",
    "    plt.savefig(\"model-train_valid_losses.png\", dpi=300, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with custom critics\n",
    "\n",
    "We also define methods which use custom defined critics for sentiment classification. We use a method to define the custom critics. Then we use a method that calls the model upon each custom critic and returns the prediction on that critic. Finally, we use a method that outputs the result to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence, min_len=5):\n",
    "    # load natural language processor\n",
    "    nlp = spacy.load('en')\n",
    "    # set the module in evaluation mode\n",
    "    model.eval()\n",
    "    # tokenize given text using the defined processor\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    # pad the sentence if it has less tokens than required\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    # convert tokens to embeddings using the fit torchtext data field\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    # convert embedding list to torch tensor and load it to the available device\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    # unsqueeze tensor to make it 2D\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    # filter prediction using sigmoid\n",
    "    out = model(tensor)\n",
    "    prediction = F.sigmoid(out)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_prediction(prediction, critic):\n",
    "    message = \"negative\"\n",
    "    if prediction < 0.5:\n",
    "        message = \"positive\"\n",
    "        prediction = 1 - prediction\n",
    "    print('Label for critic {:25s}: {:7s}\\t-\\tPrediction validity probability: {:10f}'.format('\\\"'+critic+'\\\"', message, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_testing():\n",
    "    x_critic = \"This film is terrible\"\n",
    "    y_pred = predict_sentiment(x_critic)\n",
    "    filter_prediction(y_pred, x_critic)\n",
    "    x_critic = \"This film is great\"\n",
    "    y_pred = predict_sentiment(x_critic)\n",
    "    filter_prediction(y_pred, x_critic)\n",
    "    x_critic = \"I loved this film\"\n",
    "    y_pred = predict_sentiment(x_critic)\n",
    "    filter_prediction(y_pred, x_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Binding\n",
    "\n",
    "We now bind all the methods defined above to perform (binary) sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deactivate any user Deprecation warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed random generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a seed for the randomizers\n",
    "SEED = 42\n",
    "# seed random package\n",
    "random.seed(SEED)\n",
    "# seed numpy\n",
    "numpy.random.seed(SEED)\n",
    "# seed pytorch\n",
    "torch.manual_seed(SEED)\n",
    "# make program controllability easier\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Natural Language Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English object at 0x0000027DA9F9FD00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load English package of spacy package\n",
    "spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for a GPU for faster model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any CUDA device available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the input's filepath\n",
    "dataset_filepath = \"../dataset/MoviesDataset.csv\"\n",
    "# load the dataset\n",
    "dataset = pandas.read_csv(dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect vocabulary\n",
    "vocab_subsampled, token_count = inspect_vocab(dataset)\n",
    "# set subsampling flag\n",
    "subsampling = True\n",
    "# set vocabulary size\n",
    "vocab_size = compute_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preprocessed\n",
    "dataset_preprocessor(dataset, 'Summary', \"../dataset/MoviesDatasetPreprocessed.csv\")\n",
    "# reload dataset after preprocessing\n",
    "dataset = pandas.read_csv(\"../dataset/MoviesDatasetPreprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size\n",
    "BATCH_SIZE = 32\n",
    "# split the given dataset\n",
    "train_validate_test_split(dataset, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare the pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define torchtext data text field\n",
    "TEXT = data.Field(tokenize='spacy', batch_first=True)\n",
    "# define torchtext data label field\n",
    "LABEL = data.Field(dtype=torch.float, unk_token=None, pad_token=None)\n",
    "# associate defined fields with DataFrame columns\n",
    "fields = [('Summary', TEXT), ('Sentiment', LABEL)]\n",
    "# define a dataset of columns stored in CSV\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "    path='./',\n",
    "    train='train_df.csv',\n",
    "    validation='valid_df.csv',\n",
    "    test='test_df.csv',\n",
    "    format='csv',\n",
    "    fields=fields,\n",
    "    skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the Vocab object for the TEXT field\n",
    "TEXT.build_vocab(train_data, valid_data, test_data,\n",
    "                 max_size=vocab_size,\n",
    "                 vectors=\"glove.6B.100d\",\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "# construct the Vocab object for the LABEL field\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an iterator that batches the training dataset object\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    ")\n",
    "# define an iterator that batches the validation dataset object\n",
    "valid_iterator = data.BucketIterator(\n",
    "    valid_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    ")\n",
    "# define an iterator that batches the test dataset object\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size of the dictionary of embeddings\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "# define the size of each embedding vector\n",
    "EMBEDDING_DIM = 100\n",
    "# define the number of channels produced by each convolution\n",
    "N_FILTERS = 64\n",
    "# define the size of the first dimension of the kernel of each convolutional layer\n",
    "FILTER_SIZES = [2, 3, 4, 5]\n",
    "# define the number of neurons in the output layer of the model\n",
    "OUTPUT_DIM = 1\n",
    "# define the probability of an element to be zeroed\n",
    "DROPOUT = 0.3\n",
    "# return the index of the string token used as padding\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "# return the index of the string token used to represent Out-Of-Vocabulary words\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a CNN model\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding): Embedding(4250, 100, padding_idx=1)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 64, kernel_size=(2, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 64, kernel_size=(3, 100), stride=(1, 1), padding=(1, 0))\n",
      "    (2): Conv2d(1, 64, kernel_size=(4, 100), stride=(1, 1), padding=(2, 0))\n",
      "    (3): Conv2d(1, 64, kernel_size=(5, 100), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 515,113 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# print model trainable parameters\n",
    "print(f'The model has {count_parameters():,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pretrained vectors\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "# copy pretrained vectors to the embedding layer of the defined model\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "# set the weight of the <pad> token\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "# set the weight of the <unk> token\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model's hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# define cost function\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.ones([BATCH_SIZE]))\n",
    "# load model to the available device\n",
    "model = model.to(device)\n",
    "# load cost function to the available device\n",
    "criterion = criterion.to(device)\n",
    "# define number of epochs for the model's training\n",
    "N_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a register that holds the best validation cost returned during an epoch\n",
    "best_valid_loss = float('inf')\n",
    "# declare the train and validation loss lists\n",
    "train_losses, val_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada18893796f4b4cab2fad7358c50754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Fit'), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef640e9097d4b7ca0e78b592d5d7d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0c276744a0495da42b3709c2407668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.675 | Train Acc: 58.43%\n",
      "\t Val. Loss: 0.649 |  Val. Acc: 69.39%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25031de67fa482b9b0e284590c53dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1731c0f0623d42fb933a3a490b5db176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 02 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.628 | Train Acc: 68.70%\n",
      "\t Val. Loss: 0.610 |  Val. Acc: 70.96%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add5996bc3534c1bb24e4d5bf05b2f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0014b1812eda43e0b59a580030286296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 03 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.584 | Train Acc: 71.89%\n",
      "\t Val. Loss: 0.582 |  Val. Acc: 70.59%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3278383b4434fdea298523ba4350ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8271240b5cee4686b57734ff6950d200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.545 | Train Acc: 74.47%\n",
      "\t Val. Loss: 0.553 |  Val. Acc: 73.16%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd8ed34dc8842caaab626ca8b3805f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e353bd7f10f4a6dbcb6ab442e9a6331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.512 | Train Acc: 76.31%\n",
      "\t Val. Loss: 0.536 |  Val. Acc: 74.54%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4d8022806e4f2f82bcd2e73ad33365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b012ea4fe0d4b829df79c00c3c844a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 06 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.481 | Train Acc: 78.04%\n",
      "\t Val. Loss: 0.525 |  Val. Acc: 75.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480014b3cf824501b103f9031604e5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6284008e867d4f7a9ac0227e28d970f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.458 | Train Acc: 79.46%\n",
      "\t Val. Loss: 0.514 |  Val. Acc: 75.55%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd45b350e24a4f60b59d0b49930e190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519b7e08e50c4bc7883ca2371716a462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.437 | Train Acc: 80.45%\n",
      "\t Val. Loss: 0.509 |  Val. Acc: 75.46%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ca4bb4c00647c581446d0a0ac2b738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7544a9e0e3ba451b8ac1cccf8eeefc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 09 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.416 | Train Acc: 81.82%\n",
      "\t Val. Loss: 0.503 |  Val. Acc: 76.19%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bac0a72b0054446b89e4ab530346302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cf8b0a333f4b1dbd65e1888be561a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 10 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.396 | Train Acc: 83.04%\n",
      "\t Val. Loss: 0.501 |  Val. Acc: 75.64%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99a4739bf8247428f7c530f4e63b8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91771f0f4229427d970deddc73b1d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 11 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.379 | Train Acc: 84.07%\n",
      "\t Val. Loss: 0.499 |  Val. Acc: 76.29%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c37b08343a94dd89294dea3ce5d15e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06197c4b65a4533b773f6dcb4326d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 12 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.361 | Train Acc: 84.87%\n",
      "\t Val. Loss: 0.496 |  Val. Acc: 76.38%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1218ae6183374e399d7e2657d3598705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023b2f9ee581442ca89fd40ade3e0f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 13 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.345 | Train Acc: 85.72%\n",
      "\t Val. Loss: 0.496 |  Val. Acc: 76.47%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90025f0dc942f6b2102c7fb1f8ddd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae9df87d9974efc8faee2657ce30649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 14 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.327 | Train Acc: 86.82%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 77.02%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad8a8dd7b524988976eded50e25b043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef20cfc5a0b4e14828271735d16c8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 15 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.315 | Train Acc: 87.55%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 76.47%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd46f8c7d694cf5929b62249b5a7ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f602e6bae2b40eeb9c666a6738a7817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 16 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.298 | Train Acc: 88.78%\n",
      "\t Val. Loss: 0.497 |  Val. Acc: 77.67%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981bf7af44c74eed9a9968b9158cfcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76223d4b925243a08f725c23ea265d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 17 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.285 | Train Acc: 89.28%\n",
      "\t Val. Loss: 0.500 |  Val. Acc: 77.11%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8435f0e822464562974a5f8dcfe5d795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9435d441f344a8ae45de77fc0f0aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 18 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.266 | Train Acc: 89.65%\n",
      "\t Val. Loss: 0.500 |  Val. Acc: 77.11%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab056bca363747bb862eca85d5b1cfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6722a51ef25149ebb19db64cb64d6867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 19 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.258 | Train Acc: 90.56%\n",
      "\t Val. Loss: 0.505 |  Val. Acc: 76.29%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4fee75da4c416b8777ca39a76d1bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=234.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcef5d89a1348b99eee79f43f742cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=34.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 20 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.240 | Train Acc: 90.96%\n",
      "\t Val. Loss: 0.504 |  Val. Acc: 76.56%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "for epoch in tnrange(N_EPOCHS, desc='Fit'):\n",
    "    # initialize an epoch starting time-point\n",
    "    start_time = time.time()\n",
    "    # train the model\n",
    "    train_loss, train_acc = train(train_iterator)\n",
    "    # update the train loss list\n",
    "    train_losses.append(train_acc)\n",
    "    # validate the model\n",
    "    valid_loss, valid_acc = evaluate(valid_iterator)\n",
    "    # update the validation loss list\n",
    "    val_losses.append(valid_acc)\n",
    "    # initialize an epoch ending time-point\n",
    "    end_time = time.time()\n",
    "    # compute epoch duration in minutes and seconds\n",
    "    epoch_mins, epoch_secs = epoch_time()\n",
    "    # save the model if validation loss was better than past validation losses\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'cnn-model.pt')\n",
    "    # print epoch's progress results\n",
    "    print(f'\\nEpoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model's fitting data\n",
    "plot_loss_and_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7315fa943294e0986107f65fcb82085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validate'), FloatProgress(value=0.0, max=65.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test Loss: 0.533 | Test Acc: 75.00%\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_loss, test_acc = evaluate(test_iterator)\n",
    "# print test results\n",
    "print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for critic \"This film is terrible\"  : negative\t-\tPrediction validity probability:   0.871342\n",
      "Label for critic \"This film is great\"     : positive\t-\tPrediction validity probability:   0.790964\n",
      "Label for critic \"I loved this film\"      : positive\t-\tPrediction validity probability:   0.883998\n"
     ]
    }
   ],
   "source": [
    "# test the model over custom critics\n",
    "manual_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
